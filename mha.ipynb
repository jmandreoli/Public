{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8706deb7-2d55-490e-a1de-9661c5b4d1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def randomize(*l,scale=10): # randomize the content of each tensor in l\n",
    "  for x in l: x[...] = scale*torch.rand(1)*torch.rand(x.shape)\n",
    "def div(u,v): return torch.amax(abs(v-u)).item() # divergence between two tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c231f3-794a-495e-9370-89f1dd7b6ee2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Overparametrisation of torch.nn.MultiheadAttention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5462952f-d713-4ce8-99bf-38599c1d2550",
   "metadata": {
    "tags": []
   },
   "source": [
    "The purpose of this snippet is to show that the pytorch implementation of [MultiheadAttention](https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html) is overparametrized in its management of biases.\n",
    "\n",
    "The bias for the query, key and value are held in attribute `in_proj_bias` (present by default, can be disabled by passing `bias=False` in the invocation).\n",
    "\n",
    "The snippet shows that modifying the key bias does not change the result. Changing the other two biases does change the result.\n",
    "\n",
    "This can be shown from the formula (although it is not explicitly given in the doc, one has to guess it) using the fact that softmax is invariant to an additive constant.\n",
    "\n",
    "The practical consequence is that it spends some time at each iteration both\n",
    "* forward, involving a parameter which does not change the result and\n",
    "* backward, propagating null gradients to update it.\n",
    "\n",
    "Probably negligible in the tsunami of propagations which take place, still could be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f21745f4-1c95-4270-b04e-1bb16028f43f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: (64, 80, 12)\n",
      "query : 20\n",
      "key   : 0.00092\n",
      "value : 6.3\n"
     ]
    }
   ],
   "source": [
    "# Create a torch.nn.MultiheadAttention instance and randomize all its parameters\n",
    "a = torch.nn.MultiheadAttention(embed_dim=12,num_heads=4,batch_first=True)\n",
    "#a = torch.nn.MultiheadAttention(embed_dim=12,num_heads=4,batch_first=True,kdim=17,vdim=19) # variant with key and value dimensions\n",
    "for p in a.parameters(): randomize(p.data)\n",
    "# Sample some input\n",
    "B = 64; M = 100; N = 80\n",
    "yʹ = torch.rand(B,N,a.embed_dim) # query input\n",
    "xʹ = torch.rand(B,M,a.kdim) # key input\n",
    "x =  torch.rand(B,M,a.vdim) # value input\n",
    "outs = (B,N,a.embed_dim) # shape of the output\n",
    "print(f'Output shape: {outs}')\n",
    "with torch.no_grad():\n",
    "  # iterate over the three biases (query,key,value)\n",
    "  for bias_name,bias in zip(('query','key','value'),torch.chunk(a.in_proj_bias.data,3)):\n",
    "    # First randomize the selected bias and compute the output.\n",
    "    randomize(bias); y1,_ = a(yʹ,xʹ,x)\n",
    "    # Now randomize the same bias again and compute the output.\n",
    "    randomize(bias); y2,_ = a(yʹ,xʹ,x)\n",
    "    assert y1.shape == outs and y2.shape == outs # sanity check\n",
    "    # compare the two outputs\n",
    "    print(bias_name.ljust(5),':',f'{div(y1,y2):.2g}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba44e08-cd35-4b9c-9909-c5467f80391c",
   "metadata": {},
   "source": [
    "The result for the key bias should be null, non null value is due to numerical instability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021c4768-479e-4f63-8d80-31b236b444a9",
   "metadata": {},
   "source": [
    "## Alternative implementation of multi-head attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fac63a-7198-4c82-ad82-5c9820cabf5f",
   "metadata": {},
   "source": [
    "The [PYTOOLS implementation](https://github.com/jmandreoli/PYTOOLS/blob/master/src/torch.py) of multi-head attention does not have a key bias. Furthermore, it is open to extensions of the attention mechanism. One extension (Mixed attention) is provided.\n",
    "\n",
    "This snippet shows that the implementation is equivalent (up to numerical instability) to the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd181e65-8b9d-4528-bb48-055a5f583c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward        : 0.023\n",
      "backward Λ[1]  : 1.8e-05\n",
      "         Λ[0]  : 4.1e-05\n",
      "         ϴ[0]  : 3.1e-06\n",
      "         ϴ[1]  : 1.7e-06\n",
      "         Λₒ    : 3e-05\n",
      "         Θₒ[0] : 4.5e-06\n",
      "         Θₒ[1] : 0\n"
     ]
    }
   ],
   "source": [
    "# Create a torch.nn.MultiheadAttention instance and randomize all its parameters\n",
    "#a = torch.nn.MultiheadAttention(embed_dim=12,num_heads=4,bias=True,batch_first=True)\n",
    "for p in a.parameters(): randomize(p.data)\n",
    "# Create the corresponding PYTOOLS implementation\n",
    "from myutil.torch import MultiHeadAttention\n",
    "a_ = MultiHeadAttention.torch_convert(a)\n",
    "# Compare the two implementation on some random sample\n",
    "fwd,bwd = a_.torch_compare(a,B=64,M=100,N=80)\n",
    "# display result\n",
    "print('forward'.ljust(8),''.ljust(5),':',f'{div(*fwd):.2g}')\n",
    "for head,(p,(u_,u)) in zip(('backward',*100*('',)),bwd.items()): print(head.ljust(8),p.ljust(5),':',f'{div(u_,u):.2g}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22a4670-e362-4be3-a0bf-da2339f6258e",
   "metadata": {},
   "source": [
    "All results should be null, non null values are due to numerical instability."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
